---
title: Data Lakes
metaTitle: "Data Lakes: Data Lake Hydration with Streamdal"
description: Streamdal hydrates and manages your data lakes
layout: ../../../layouts/MainLayout.astro
---

Data lakes are a collection of structured, semi-structured or unstructured data that you do not know the exact purpose of at the time of collection. Where a data warehouse is a carefully constructed SQL database that serves a specific data-analytics purpose, a data lake is the step _before_ a data warehouse - you collect data that you _may_ want to use in the future.

One huge difference between data lakes and data warehouses is that data lakes may contain terabytes or petabytes of data while a data warehouse will be _much_ smaller in size.

For example, your data lake may contain ALL of your traffic logs while a data warehouse may contain only order fulfillment data.

An important characteristics of a data lake is that it must be _highly scalable_ - as in, you should be able to store a near-infinite amount of data in it and be able to retrieve it within a reasonable amount of time (seconds or minutes, rather than days or weeks).

This usually boils down to the following:

- Data lakes should be powered by "storage and search" oriented tech

  - [AWS S3](https://aws.amazon.com/s3/) + [Athena](https://aws.amazon.com/athena/)
  - [Presto](https://prestodb.io/) + [HDFS](https://hadoop.apache.org/)

- Data should be stored in a search friendly format such as Parquet

Streamdal satisfies **both** of these requirements by writing **all** ingested data as optimized parquet files into an S3 bucket.

## Data lake hydration

Data lake hydration is the act of populating your data lake with data.

There are many ways to get data into a data lake:

1. You can setup pipelines to push data from your system components (such as Kafka) into your data lake of choice
2. You can write scripts that fetch and push data to your data lake
3. You can setup various integrations from other vendors (such as Segment) to push analytics data into your data lake

**Streamdal's data lake hydration removes the necessity for doing #1 and #2.**

Upon receiving an event, Streamdal will automatically:

- Generate a parquet and Athena schema for your events
- Generate optimally partitioned and sized parquet files
- Streamdal write the generated files to your S3 bucket

## Using a custom data lake

By default, Streamdal will write all ingested events into its own, internal S3 bucket.

This might be OK if you do _not_ require access to the parquet files or simply do not want to manage additional AWS resources.

However, if you have requirements for the data to reside in **your** S3 bucket and/or you would like your teams to have access to the parquet data, you can do so as follows:

1. **Create an IAM policy that allows our AWS account to read, write and list files in your S3 bucket:**

[https://docs.aws.amazon.com/athena/latest/ug/cross-account-permissions.html](https://docs.aws.amazon.com/athena/latest/ug/cross-account-permissions.html)

```javascript
{
   "Version": "2012-10-17",
   "Id": "StreamdalS3AccessPolicyID",
   "Statement": [
      {
          "Sid": "MyStatementSid",
          "Effect": "Allow",
          "Principal": {
             "AWS": "arn:aws:iam::589147263245:root"
          },
          "Action": [
             "s3:GetBucketLocation",
             "s3:GetObject",
             "s3:ListBucket",
             "s3:ListBucketMultipartUploads",
             "s3:ListMultipartUploadParts",
             "s3:AbortMultipartUpload",
             "s3:PutObject",
             "s3:DeleteObject"
          ],
          "Resource": [
             "arn:aws:s3:::my-athena-data-bucket",
             "arn:aws:s3:::my-athena-data-bucket/*"
          ]
       }
    ]
 }
```

    2. **Create a collection that uses **_**your**_** S3 bucket:**

![](files/-McMkp-jNdWFKPtoThHQ.png)

    3. **You're done!**

Once all of the above is done, Streamdal will store all parquet data in your S3 bucket, using the following "directory" structure:

```bash
s3://your-bucket/batch-collections/{{collection_id}}/year=XXXX/month=XX/day=XX/*.parquet
```

## Best practices

1. **Do not modify the bucket contents**
1. While you have complete access and control over all of the parquet data, modifying the structure (or contents) of the bucket can affect our ability to replay data.

1. **Modifying the IAM S3 policy _may_ affect our ability to write data**
1. If you modify the IAM policy, make sure that new collected events continue to be written to S3. If not, [contact us](https://streamdal.com/contact).
