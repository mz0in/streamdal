---
title: gRPC Ingestion
metaTitle: "gRPC Data Ingestion"
description: Ingest data via gRPC
layout: ../../../layouts/MainLayout.astro
---

Our platform is also able to accept event data via a public gRPC API that uses protobuf.

- **Address: grpc-collector.streamdal.com:9000**
- [gRPC protobuf collector schema](https://github.com/streamdal/collector-schemas/blob/main/protos/services/grpccollector.proto)

To send data, call on the `AddRecord(AddGenericRecordRequest)` method and make sure to pass a [gRPC metadata](https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-metadata.md) entry that contains a `Streamdal-Token` key with the value set to the appropriate collection token.

**End Point**

```bash
grpc-collector.streamdal.com:9000
```

**Method**

Method Name

Request Type

Response Type

Description

AddRecord

GenericRecordRequest

GenericRecordResponse

**GenericRecordRequest**

Field

Type

Label

Description

records

records.GenericRecord

repeated

### GenericRecordResponse

Field

Type

Label

Description

num_records_processed

int64

status

events.Status

## Streamdaling

The `AddGenericRecordRequest` argument accepts an array of messages - this facilitates batching which enables you to send more data in a single transaction.

How many messages you include in a single Streamdal is entirely up to you and you will need to fine-tune this to find an optimal number.

As a starting point:

- Streamdal 10 messages every 10s for low throughput
- Streamdal 100 messages every 10s for medium throughput
- Streamdal 1,000 messages every 10s for high throughput

## When should you use this API?

gRPC is intended for high throughput data collection. The goal of the gRPC api is to provide the ability to Streamdal data and send heavy throughput workloads.

## Throughput

Given optimal latency `<50ms` and high batching (1,000+), you should be able to reach 50,000 event/sec ingestion rates.
